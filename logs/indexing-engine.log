2025-10-09 21:58:59.0140 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125746, Requested 18. Please try again in 6h11m0.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125746, Requested 18. Please try again in 6h11m0.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125746, Requested 18. Please try again in 6h11m0.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:58:59.0348 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h11m0.002s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h11m0.002s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h11m0.002s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:58:59.0577 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:58:59.0764 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.578s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.578s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.578s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:59:00.0077 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.289s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.289s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125745, Requested 18. Please try again in 6h10m59.289s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:59:00.0299 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:59:00.0305 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 21:59:00.0377 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125744, Requested 18. Please try again in 6h10m59.049s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:49.0644 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125479, Requested 18. Please try again in 6h7m9.692s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125479, Requested 18. Please try again in 6h7m9.692s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125479, Requested 18. Please try again in 6h7m9.692s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:49.0846 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125479, Requested 18. Please try again in 6h7m9.487s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125479, Requested 18. Please try again in 6h7m9.487s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125479, Requested 18. Please try again in 6h7m9.487s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:50.0002 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m9.322999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m9.322999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m9.322999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:50.0227 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m9.13s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m9.13s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m9.13s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:50.0395 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.938s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.938s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.938s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:50.0574 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:50.0577 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:02:50.0624 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125478, Requested 18. Please try again in 6h7m8.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:32.0251 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m27.164s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m27.164s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m27.164s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:32.0484 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m26.874s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m26.874s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m26.874s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:32.0730 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m26.661s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m26.661s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125082, Requested 18. Please try again in 6h1m26.661s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:32.0971 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m26.372s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m26.372s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m26.372s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:33.0276 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m26.116s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m26.116s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m26.116s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:33.0660 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:33.0667 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:08:33.0756 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 125081, Requested 18. Please try again in 6h1m25.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:28.0039 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m31.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m31.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m31.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:28.0273 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m31.087s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m31.087s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m31.087s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:28.0511 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m30.846s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m30.846s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124601, Requested 18. Please try again in 5h54m30.846s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:28.0758 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.589s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.589s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.589s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:28.0982 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.359s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.359s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.359s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:29.0232 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:29.0238 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:15:29.0351 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 124600, Requested 18. Please try again in 5h54m30.108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:48.0593 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.748s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.748s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.748s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:48.0761 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.593s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.593s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.593s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:48.0888 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.435999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.435999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.435999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:49.0011 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.33s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.33s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.33s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:49.0116 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.212s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.212s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.212s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:49.0232 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:49.0236 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:37:49.0244 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 123049, Requested 18. Please try again in 5h32m10.091s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:50.0524 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.83s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.83s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.83s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:50.0655 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.691s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.691s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.691s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:50.0772 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.563s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.563s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122978, Requested 18. Please try again in 5h31m8.563s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:50.0892 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:51.0009 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.329s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.329s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.329s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:51.0130 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:51.0134 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:38:51.0144 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 122977, Requested 18. Please try again in 5h31m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:03.0499 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121991, Requested 18. Please try again in 5h16m55.826s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121991, Requested 18. Please try again in 5h16m55.826s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121991, Requested 18. Please try again in 5h16m55.826s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:03.0637 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.678s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.678s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.678s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:03.0791 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.54s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.54s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.54s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:03.0908 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.427s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.427s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.427s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:04.0029 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.309999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.309999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.309999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:04.0145 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:04.0149 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:53:04.0157 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121990, Requested 18. Please try again in 5h16m55.19s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:14.0481 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.843999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.843999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.843999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:14.0579 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.741s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.741s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.741s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:14.0684 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.634s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.634s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.634s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:14.0808 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.537s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.537s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121839, Requested 18. Please try again in 5h14m44.537s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:14.0921 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.413s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.413s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.413s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:15.0036 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:15.0041 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:55:15.0049 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121838, Requested 18. Please try again in 5h14m44.302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:09.0901 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.425s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.425s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.425s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0013 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.303s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.303s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.303s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0119 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.198999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.198999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121775, Requested 18. Please try again in 5h13m49.198999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0215 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m49.101s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m49.101s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m49.101s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0327 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.988s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.988s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.988s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0447 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0451 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "D:\Python\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
  File "D:\Python\lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
  File "D:\Python\lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 108, in _base_acompletion
    return await acompletion(**new_args)
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "D:\Python\lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
  File "D:\Python\lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "D:\Python\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 22:56:10.0457 - ERROR - graphrag.index.validate_config - LLM configuration error detected. Exiting...
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 121774, Requested 18. Please try again in 5h13m48.889s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:03.0464 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119630, Requested 18. Please try again in 4h42m55.879s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119630, Requested 18. Please try again in 4h42m55.879s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119630, Requested 18. Please try again in 4h42m55.879s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:03.0565 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.754s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.754s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.754s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:03.0681 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.646999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.646999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.646999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:03.0814 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.503s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.503s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.503s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:03.0921 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.409s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.409s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.409s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:04.0033 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:04.0039 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 148, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 330, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:27:04.0050 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k75zjwbpegjtg37642hnqyt6` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 119629, Requested 18. Please try again in 4h42m55.301s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-10-09 23:56:29.0894 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2025-10-09 23:56:29.0947 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:29.0997 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0033 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0076 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0102 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0129 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=6, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0153 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=7, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0183 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=8, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0210 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=9, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0235 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=10, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0261 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=10, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0268 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:56:30.0279 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.APIConnectionError: OllamaException - Client error '404 Not Found' for url 'http://localhost:11434/api/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-10-09 23:57:58.0246 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2025-10-09 23:57:58.0289 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0317 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0347 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0370 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0398 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0424 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=6, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0449 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=7, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0478 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=8, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0502 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=9, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0530 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=10, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0554 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=10, max_retries=10, exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0561 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3845, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\ollama\completion\handler.py", line 87, in ollama_aembeddings
    response = await litellm.module_level_aclient.post(url=api_base, json=data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 328, in post
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 284, in post
    response.raise_for_status()
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\embedding_model.py", line 95, in _base_aembedding
    return await aembedding(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 3860, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-09 23:57:58.0571 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.APIConnectionError: OllamaException - Client error '405 Method Not Allowed' for url 'http://localhost:11434/api/embed'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405
2025-10-10 00:01:25.0826 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2025-10-10 00:01:29.0246 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2025-10-10 00:01:29.0246 - INFO - graphrag.cli.index - Starting pipeline run. False
2025-10-10 00:01:29.0247 - INFO - graphrag.cli.index - Using default configuration: {
    "root_dir": "C:\\Users\\zhour\\Desktop\\christmas",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "chat",
            "model_provider": "ollama",
            "model": "mistral",
            "encoding_model": "",
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": null,
            "requests_per_minute": null,
            "rate_limit_strategy": "static",
            "retry_strategy": "native",
            "max_retries": 5,
            "max_retry_wait": 10.0,
            "concurrent_requests": 5,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "embedding",
            "model_provider": "ollama",
            "model": "nomic-embed-text",
            "encoding_model": "",
            "api_base": "http://localhost:11434",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "request_timeout": 180.0,
            "tokens_per_minute": null,
            "requests_per_minute": null,
            "rate_limit_strategy": "static",
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 5,
            "async_mode": "threaded",
            "responses": null,
            "max_tokens": null,
            "temperature": 0,
            "max_completion_tokens": null,
            "reasoning_effort": null,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        }
    },
    "input": {
        "storage": {
            "type": "file",
            "base_dir": "C:\\Users\\zhour\\Desktop\\christmas\\input",
            "storage_account_blob_url": null,
            "cosmosdb_account_url": null
        },
        "file_type": "text",
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhour\\Desktop\\christmas\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhour\\Desktop\\christmas\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\zhour\\Desktop\\christmas\\logs",
        "storage_account_blob_url": null
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\zhour\\Desktop\\christmas\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true,
            "embeddings_schema": {}
        }
    },
    "workflows": null,
    "embed_text": {
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity.description",
            "community.full_content",
            "text_unit.text"
        ],
        "strategy": null
    },
    "extract_graph": {
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null
    },
    "summarize_descriptions": {
        "model_id": "default_chat_model",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000,
        "strategy": null
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 4,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "model_id": "default_chat_model",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "community_reports": {
        "model_id": "default_chat_model",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "umap": {
        "enabled": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2025-10-10 00:01:29.0251 - INFO - graphrag.api.index - Initializing indexing pipeline...
2025-10-10 00:01:29.0251 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2025-10-10 00:01:29.0251 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\input
2025-10-10 00:01:29.0252 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\output
2025-10-10 00:01:29.0253 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas
2025-10-10 00:01:29.0255 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\cache
2025-10-10 00:01:29.0259 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2025-10-10 00:01:29.0259 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at 
2025-10-10 00:01:29.0264 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2025-10-10 00:01:29.0265 - INFO - graphrag.index.input.factory - loading input from root_dir=C:\Users\zhour\Desktop\christmas\input
2025-10-10 00:01:29.0265 - INFO - graphrag.index.input.factory - Loading Input InputFileType.text
2025-10-10 00:01:29.0265 - INFO - graphrag.storage.file_pipeline_storage - search C:\Users\zhour\Desktop\christmas\input for files matching .*\.txt$
2025-10-10 00:01:29.0275 - INFO - graphrag.index.input.util - Found 1 InputFileType.text files, loading 1
2025-10-10 00:01:29.0276 - INFO - graphrag.index.input.util - Total number of unfiltered InputFileType.text rows: 1
2025-10-10 00:01:29.0276 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2025-10-10 00:01:29.0323 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2025-10-10 00:01:29.0343 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2025-10-10 00:01:29.0345 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-10-10 00:01:29.0384 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2025-10-10 00:01:29.0846 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2025-10-10 00:01:29.0869 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2025-10-10 00:01:29.0870 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2025-10-10 00:01:29.0886 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2025-10-10 00:01:29.0887 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2025-10-10 00:01:29.0900 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-10-10 00:01:29.0932 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2025-10-10 00:01:29.0932 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2025-10-10 00:01:29.0942 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2025-10-10 00:01:29.0944 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-10-10 00:01:29.0973 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\cache\extract_graph
2025-10-10 00:04:31.0065 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 00:04:31.0112 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.194 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.194 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.194 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.194 seconds
2025-10-10 00:04:31.0212 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds
2025-10-10 00:04:31.0255 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.333 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.333 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.333 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.333 seconds
2025-10-10 00:05:25.0490 - INFO - graphrag.logger.progress - extract graph progress: 1/42
2025-10-10 00:07:31.0180 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:07:31.0317 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 00:07:31.0474 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds
2025-10-10 00:07:31.0578 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
2025-10-10 00:08:25.0549 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 00:10:31.0249 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 00:10:31.0523 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 00:10:31.0612 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.263 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.263 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.263 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.263 seconds
2025-10-10 00:10:31.0891 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
2025-10-10 00:11:25.0654 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:13:31.0348 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
2025-10-10 00:13:31.0695 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
2025-10-10 00:13:31.0918 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.287 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.287 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.287 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.287 seconds
2025-10-10 00:13:32.0208 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds
2025-10-10 00:14:25.0744 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds
2025-10-10 00:16:31.0445 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:16:31.0982 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:16:32.0068 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.304 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.304 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.304 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.304 seconds
2025-10-10 00:16:32.0528 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
2025-10-10 00:17:25.0845 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 00:19:31.0541 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:19:31.0565 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:19:31.0602 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:19:31.0650 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 00:19:31.0670 - INFO - graphrag.logger.progress - extract graph progress: 2/42
2025-10-10 00:19:32.0141 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 00:19:32.0151 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 00:19:32.0163 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 00:19:32.0187 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 00:19:32.0200 - INFO - graphrag.logger.progress - extract graph progress: 3/42
2025-10-10 00:19:32.0409 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds
2025-10-10 00:19:32.0419 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds
2025-10-10 00:19:32.0431 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds
2025-10-10 00:19:32.0459 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.279 seconds
2025-10-10 00:19:32.0472 - INFO - graphrag.logger.progress - extract graph progress: 4/42
2025-10-10 00:19:32.0845 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
2025-10-10 00:19:32.0856 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
2025-10-10 00:19:32.0866 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
2025-10-10 00:19:32.0889 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
2025-10-10 00:19:32.0901 - INFO - graphrag.logger.progress - extract graph progress: 5/42
2025-10-10 00:20:25.0924 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
2025-10-10 00:22:31.0732 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 00:22:32.0527 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 00:22:32.0602 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.346 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.346 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.346 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.346 seconds
2025-10-10 00:22:33.0204 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds
2025-10-10 00:23:26.0006 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 00:23:26.0050 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 00:23:26.0077 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 00:23:26.0117 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 00:23:26.0132 - INFO - graphrag.logger.progress - extract graph progress: 6/42
2025-10-10 00:25:31.0944 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.033 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.033 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.033 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.033 seconds
2025-10-10 00:25:32.0787 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 00:25:32.0982 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
2025-10-10 00:25:33.0589 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.28 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.28 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.28 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.28 seconds
2025-10-10 00:28:32.0087 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.071 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.071 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.071 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.071 seconds
2025-10-10 00:28:33.0061 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 00:28:33.0145 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.286 seconds
2025-10-10 00:28:33.0938 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
2025-10-10 00:28:52.0954 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 00:31:32.0158 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 00:31:33.0194 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 00:31:33.0365 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds
2025-10-10 00:31:34.0254 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.278 seconds
2025-10-10 00:31:53.0062 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 00:34:32.0451 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.022 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.022 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.022 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.022 seconds
2025-10-10 00:34:33.0487 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:34:33.0660 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.329 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.329 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.329 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.329 seconds
2025-10-10 00:34:34.0643 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.271 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.271 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.271 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.271 seconds
2025-10-10 00:34:53.0169 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 00:37:32.0548 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 00:37:32.0560 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 00:37:32.0575 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 00:37:32.0608 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 00:37:32.0640 - INFO - graphrag.logger.progress - extract graph progress: 7/42
2025-10-10 00:37:33.0754 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:37:33.0763 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:37:33.0774 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:37:33.0801 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:37:33.0815 - INFO - graphrag.logger.progress - extract graph progress: 8/42
2025-10-10 00:37:34.0022 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds
2025-10-10 00:37:34.0030 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds
2025-10-10 00:37:34.0040 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds
2025-10-10 00:37:34.0063 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.285 seconds
2025-10-10 00:37:34.0076 - INFO - graphrag.logger.progress - extract graph progress: 9/42
2025-10-10 00:37:34.0989 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
2025-10-10 00:37:34.0999 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
2025-10-10 00:37:35.0009 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
2025-10-10 00:37:35.0031 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.282 seconds
2025-10-10 00:37:35.0043 - INFO - graphrag.logger.progress - extract graph progress: 10/42
2025-10-10 00:37:53.0269 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
2025-10-10 00:40:32.0739 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 00:40:34.0099 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds
2025-10-10 00:40:34.0124 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 00:40:35.0353 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
2025-10-10 00:40:53.0348 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 00:43:32.0813 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 00:43:34.0170 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 00:43:34.0427 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds
2025-10-10 00:43:35.0644 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds
2025-10-10 00:43:53.0428 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 00:43:53.0450 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 00:43:53.0479 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 00:43:53.0528 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 00:43:53.0563 - INFO - graphrag.logger.progress - extract graph progress: 11/42
2025-10-10 00:46:32.0857 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
2025-10-10 00:46:34.0476 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.276 seconds
2025-10-10 00:46:34.0504 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds
2025-10-10 00:46:35.0938 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.259 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.259 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.259 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.259 seconds
2025-10-10 00:46:53.0825 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.131 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.131 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.131 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.131 seconds
2025-10-10 00:49:32.0955 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 00:49:34.0567 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 00:49:34.0819 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
2025-10-10 00:49:36.0256 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.281 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.281 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.281 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.281 seconds
2025-10-10 00:49:53.0993 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 00:52:33.0080 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 00:52:34.0883 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.257 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.257 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.257 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.257 seconds
2025-10-10 00:52:34.0954 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.076 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.076 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.076 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.076 seconds
2025-10-10 00:52:36.0591 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.261 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.261 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.261 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.261 seconds
2025-10-10 00:52:54.0142 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 00:55:33.0262 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 00:55:33.0281 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 00:55:33.0299 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 00:55:33.0333 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 00:55:33.0357 - INFO - graphrag.logger.progress - extract graph progress: 12/42
2025-10-10 00:55:35.0029 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 00:55:35.0066 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 00:55:35.0082 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 00:55:35.0111 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 00:55:35.0132 - INFO - graphrag.logger.progress - extract graph progress: 13/42
2025-10-10 00:55:35.0250 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds
2025-10-10 00:55:35.0259 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds
2025-10-10 00:55:35.0268 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds
2025-10-10 00:55:35.0288 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.254 seconds
2025-10-10 00:55:35.0299 - INFO - graphrag.logger.progress - extract graph progress: 14/42
2025-10-10 00:55:36.0904 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
2025-10-10 00:55:36.0911 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
2025-10-10 00:55:36.0922 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
2025-10-10 00:55:36.0943 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
2025-10-10 00:55:36.0957 - INFO - graphrag.logger.progress - extract graph progress: 15/42
2025-10-10 00:55:54.0224 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 00:58:35.0355 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
2025-10-10 00:58:35.0511 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
2025-10-10 00:58:37.0245 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.26 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.26 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.26 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.26 seconds
2025-10-10 00:58:54.0294 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:00:49.0962 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:01:35.0405 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
2025-10-10 01:01:36.0264 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.709 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.709 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.709 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.709 seconds
2025-10-10 01:01:37.0292 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:01:54.0374 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:01:54.0403 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:01:54.0429 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:01:54.0478 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:01:54.0493 - INFO - graphrag.logger.progress - extract graph progress: 16/42
2025-10-10 01:03:50.0064 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds
2025-10-10 01:04:36.0327 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.045 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.045 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.045 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.045 seconds
2025-10-10 01:04:36.0562 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.269 seconds
2025-10-10 01:04:37.0345 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 01:04:54.0580 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 01:06:50.0163 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 01:07:36.0382 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 01:07:36.0853 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
2025-10-10 01:07:37.0383 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
2025-10-10 01:07:54.0664 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:09:50.0218 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 01:10:36.0479 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:10:37.0157 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.272 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.272 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.272 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.272 seconds
2025-10-10 01:10:37.0440 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 01:10:54.0729 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 01:12:50.0313 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:13:36.0589 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 01:13:36.0610 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 01:13:36.0632 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 01:13:36.0686 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 01:13:36.0708 - INFO - graphrag.logger.progress - extract graph progress: 17/42
2025-10-10 01:13:37.0495 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds
2025-10-10 01:13:37.0507 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds
2025-10-10 01:13:37.0519 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds
2025-10-10 01:13:37.0544 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.301 seconds
2025-10-10 01:13:37.0558 - INFO - graphrag.logger.progress - extract graph progress: 18/42
2025-10-10 01:13:37.0579 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds
2025-10-10 01:13:37.0590 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds
2025-10-10 01:13:37.0602 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds
2025-10-10 01:13:37.0627 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.103 seconds
2025-10-10 01:13:37.0641 - INFO - graphrag.logger.progress - extract graph progress: 19/42
2025-10-10 01:13:54.0796 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:15:50.0383 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:15:50.0396 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:15:50.0411 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:15:50.0448 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:15:50.0477 - INFO - graphrag.logger.progress - extract graph progress: 20/42
2025-10-10 01:16:36.0770 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 01:16:37.0687 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 01:16:37.0954 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
2025-10-10 01:16:54.0871 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:18:50.0561 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:19:36.0862 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:19:37.0986 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.264 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.264 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.264 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.264 seconds
2025-10-10 01:19:38.0027 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.034 seconds
2025-10-10 01:19:54.0922 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:19:54.0938 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:19:54.0954 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:19:54.0983 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:19:54.0999 - INFO - graphrag.logger.progress - extract graph progress: 21/42
2025-10-10 01:21:50.0640 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 01:22:36.0967 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 01:22:38.0097 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
2025-10-10 01:22:38.0352 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
2025-10-10 01:22:55.0051 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
2025-10-10 01:24:50.0723 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
2025-10-10 01:25:37.0100 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.035 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.035 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.035 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.035 seconds
2025-10-10 01:25:38.0421 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 01:25:38.0451 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.312 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.312 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.312 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.312 seconds
2025-10-10 01:25:55.0141 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 01:27:50.0821 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
2025-10-10 01:28:37.0261 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 01:28:38.0549 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.0 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.0 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.0 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.0 seconds
2025-10-10 01:28:38.0788 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.268 seconds
2025-10-10 01:28:55.0235 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 01:30:50.0924 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:31:37.0390 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:31:37.0427 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:31:37.0455 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:31:37.0513 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:31:37.0566 - INFO - graphrag.logger.progress - extract graph progress: 22/42
2025-10-10 01:31:38.0878 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:31:38.0903 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:31:38.0925 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:31:38.0973 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:31:38.0996 - INFO - graphrag.logger.progress - extract graph progress: 23/42
2025-10-10 01:31:39.0033 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds
2025-10-10 01:31:39.0053 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds
2025-10-10 01:31:39.0076 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds
2025-10-10 01:31:39.0121 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.401 seconds
2025-10-10 01:31:39.0143 - INFO - graphrag.logger.progress - extract graph progress: 24/42
2025-10-10 01:31:55.0640 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.163 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.163 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.163 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.163 seconds
2025-10-10 01:33:51.0046 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 01:33:51.0074 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 01:33:51.0102 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 01:33:51.0238 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 01:33:51.0279 - INFO - graphrag.logger.progress - extract graph progress: 25/42
2025-10-10 01:34:37.0671 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:34:39.0205 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:34:39.0477 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.275 seconds
2025-10-10 01:34:55.0825 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 01:36:51.0431 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
2025-10-10 01:37:37.0815 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:37:39.0557 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:37:39.0640 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.341 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.341 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.341 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.341 seconds
2025-10-10 01:37:55.0974 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:37:56.0011 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:37:56.0061 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:37:56.0199 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:37:56.0249 - INFO - graphrag.logger.progress - extract graph progress: 26/42
2025-10-10 01:39:51.0520 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 01:40:37.0878 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:40:39.0699 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
2025-10-10 01:40:39.0951 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.266 seconds
2025-10-10 01:40:56.0306 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 01:42:51.0611 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 01:43:37.0984 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:43:40.0001 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
2025-10-10 01:43:40.0084 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
2025-10-10 01:43:56.0380 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.004 seconds
2025-10-10 01:45:51.0717 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 01:46:38.0099 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 01:46:40.0160 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 01:46:40.0408 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.267 seconds
2025-10-10 01:46:56.0460 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 01:48:51.0820 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.019 seconds
2025-10-10 01:49:38.0185 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:49:38.0230 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:49:38.0292 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:49:38.0375 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 01:49:38.0401 - INFO - graphrag.logger.progress - extract graph progress: 27/42
2025-10-10 01:49:40.0438 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
2025-10-10 01:49:40.0448 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
2025-10-10 01:49:40.0459 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
2025-10-10 01:49:40.0482 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
2025-10-10 01:49:40.0495 - INFO - graphrag.logger.progress - extract graph progress: 28/42
2025-10-10 01:49:40.0515 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
2025-10-10 01:49:40.0527 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
2025-10-10 01:49:40.0540 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
2025-10-10 01:49:40.0569 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.324 seconds
2025-10-10 01:49:40.0583 - INFO - graphrag.logger.progress - extract graph progress: 29/42
2025-10-10 01:49:56.0528 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 01:51:51.0913 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:51:51.0942 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:51:51.0961 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:51:52.0006 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:51:52.0022 - INFO - graphrag.logger.progress - extract graph progress: 30/42
2025-10-10 01:52:38.0469 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 01:52:40.0634 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 01:52:40.0883 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
2025-10-10 01:52:56.0617 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 01:54:52.0133 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.044 seconds
2025-10-10 01:55:38.0586 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 01:55:40.0942 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 01:55:40.0984 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds
2025-10-10 01:55:56.0678 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:55:56.0693 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:55:56.0709 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:55:56.0742 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 01:55:56.0787 - INFO - graphrag.logger.progress - extract graph progress: 31/42
2025-10-10 01:57:52.0200 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 01:58:38.0687 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 01:58:41.0032 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 01:58:41.0303 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.284 seconds
2025-10-10 01:58:56.0900 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.02 seconds
2025-10-10 02:00:52.0286 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 02:01:38.0750 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.002 seconds
2025-10-10 02:01:41.0347 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 02:01:41.0375 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.313 seconds
2025-10-10 02:01:57.0005 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 02:03:52.0358 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
2025-10-10 02:04:40.0200 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 02:04:41.0432 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.024 seconds
2025-10-10 02:04:41.0664 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.258 seconds
2025-10-10 02:04:57.0094 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.021 seconds
2025-10-10 02:06:52.0456 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.015 seconds
2025-10-10 02:07:40.0334 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 02:07:40.0366 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 02:07:40.0388 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 02:07:40.0433 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 02:07:40.0478 - INFO - graphrag.logger.progress - extract graph progress: 32/42
2025-10-10 02:07:41.0705 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
2025-10-10 02:07:41.0717 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
2025-10-10 02:07:41.0730 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
2025-10-10 02:07:41.0761 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.012 seconds
2025-10-10 02:07:41.0778 - INFO - graphrag.logger.progress - extract graph progress: 33/42
2025-10-10 02:07:41.0816 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds
2025-10-10 02:07:41.0832 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds
2025-10-10 02:07:41.0850 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds
2025-10-10 02:07:41.0883 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.335 seconds
2025-10-10 02:07:41.0898 - INFO - graphrag.logger.progress - extract graph progress: 34/42
2025-10-10 02:07:57.0225 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 02:09:52.0527 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 02:09:52.0548 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 02:09:52.0593 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 02:09:52.0667 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 02:09:52.0702 - INFO - graphrag.logger.progress - extract graph progress: 35/42
2025-10-10 02:10:40.0582 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
2025-10-10 02:10:41.0936 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
2025-10-10 02:10:42.0210 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.262 seconds
2025-10-10 02:10:57.0330 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.01 seconds
2025-10-10 02:12:52.0797 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.031 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.031 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.031 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.031 seconds
2025-10-10 02:13:40.0714 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 02:13:42.0265 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.293 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.293 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.293 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.293 seconds
2025-10-10 02:13:42.0298 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.055 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.055 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.055 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.055 seconds
2025-10-10 02:13:57.0419 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 02:13:57.0441 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 02:13:57.0468 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 02:13:57.0557 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.006 seconds
2025-10-10 02:13:57.0606 - INFO - graphrag.logger.progress - extract graph progress: 36/42
2025-10-10 02:15:52.0902 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.014 seconds
2025-10-10 02:16:40.0794 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.026 seconds
2025-10-10 02:16:42.0324 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.996 seconds
2025-10-10 02:16:42.0595 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.265 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.265 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.265 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.265 seconds
2025-10-10 02:16:57.0714 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 02:18:53.0025 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.007 seconds
2025-10-10 02:19:40.0902 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.023 seconds
2025-10-10 02:19:42.0625 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.273 seconds
2025-10-10 02:19:42.0652 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds
2025-10-10 02:19:57.0833 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.027 seconds
2025-10-10 02:21:53.0084 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.998 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.998 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.998 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=179.998 seconds
2025-10-10 02:22:41.0008 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.008 seconds
2025-10-10 02:22:42.0698 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 02:22:42.0933 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.253 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.253 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.253 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.253 seconds
2025-10-10 02:22:57.0906 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.017 seconds
2025-10-10 02:24:53.0164 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.025 seconds
2025-10-10 02:25:41.0081 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 02:25:41.0108 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 02:25:41.0157 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 02:25:41.0234 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.005 seconds
2025-10-10 02:25:41.0256 - INFO - graphrag.logger.progress - extract graph progress: 37/42
2025-10-10 02:25:42.0973 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 02:25:42.0983 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 02:25:42.0995 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 02:25:43.0017 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.013 seconds
2025-10-10 02:25:43.0029 - INFO - graphrag.logger.progress - extract graph progress: 38/42
2025-10-10 02:25:43.0047 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds
2025-10-10 02:25:43.0059 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds
2025-10-10 02:25:43.0071 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds
2025-10-10 02:25:43.0096 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.32 seconds
2025-10-10 02:25:43.0109 - INFO - graphrag.logger.progress - extract graph progress: 39/42
2025-10-10 02:25:57.0964 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 02:27:53.0265 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
2025-10-10 02:27:53.0282 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
2025-10-10 02:27:53.0311 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
2025-10-10 02:27:53.0399 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.04 seconds
2025-10-10 02:27:53.0446 - INFO - graphrag.logger.progress - extract graph progress: 40/42
2025-10-10 02:28:41.0315 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 02:28:58.0021 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.011 seconds
2025-10-10 02:31:58.0102 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 02:31:58.0128 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 02:31:58.0163 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 02:31:58.0231 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 146, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.029 seconds
2025-10-10 02:31:58.0257 - INFO - graphrag.logger.progress - extract graph progress: 41/42
2025-10-10 02:34:21.0081 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=1, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
2025-10-10 02:37:21.0243 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=2, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.028 seconds
2025-10-10 02:40:21.0319 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=3, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.009 seconds
2025-10-10 02:43:21.0403 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=4, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.016 seconds
2025-10-10 02:46:21.0498 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Request failed, immediately retrying, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.018 seconds
2025-10-10 02:49:21.0612 - ERROR - graphrag.language_model.providers.litellm.services.retry.native_wait_retry - NativeRetry: Max retries exceeded, retries=5, max_retries=5, exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds
2025-10-10 02:49:21.0641 - ERROR - graphrag.language_model.providers.litellm.request_wrappers.with_logging - with_logging: Async request failed with exception=litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds
2025-10-10 02:49:21.0672 - ERROR - graphrag.index.operations.extract_graph.graph_extractor - error extracting graph
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds
2025-10-10 02:49:21.0745 - ERROR - graphrag.index.operations.extract_graph.graph_intelligence_strategy - Entity Extraction Error
Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 59, in map_aiohttp_exceptions
    yield
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 219, in handle_async_request
    response = await client_session.request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.SocketTimeoutError: Timeout on reading data from socket

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 283, in post
    response = await self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 212, in handle_async_request
    with map_aiohttp_exceptions():
  File "C:\Program Files\Python311\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\aiohttp_transport.py", line 73, in map_aiohttp_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: Timeout on reading data from socket

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 123, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 312, in post
    raise litellm.Timeout(
litellm.exceptions.Timeout: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 578, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 255, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 150, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 3025, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.ollama.common_utils.OllamaError: litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 118, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\index\operations\extract_graph\graph_extractor.py", line 158, in _process_document
    response = await self._model.achat(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 286, in achat
    response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_logging.py", line 49, in _wrapped_with_logging_async
    return await async_fn(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_cache.py", line 103, in _wrapped_with_cache_async
    response = await async_fn(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\request_wrappers\with_retries.py", line 52, in _wrapped_with_retries_async
    return await retry_service.aretry(func=async_fn, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\native_wait_retry.py", line 56, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1612, in wrapper_async
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\utils.py", line 1462, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\main.py", line 597, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2273, in exception_type
    raise e
  File "C:\Users\zhour\Desktop\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2242, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out. Timeout passed=180.0, time taken=180.032 seconds
2025-10-10 02:49:21.0770 - INFO - graphrag.logger.progress - extract graph progress: 42/42
2025-10-10 02:49:22.0211 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\cache\summarize_descriptions
2025-10-10 02:49:22.0221 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/15
2025-10-10 02:49:22.0222 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/15
2025-10-10 02:49:22.0222 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/15
2025-10-10 02:49:22.0222 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/15
2025-10-10 02:49:22.0223 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/15
2025-10-10 02:49:22.0223 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/15
2025-10-10 02:49:22.0224 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/15
2025-10-10 02:49:22.0224 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/15
2025-10-10 02:49:22.0226 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/15
2025-10-10 02:49:22.0226 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 10/15
2025-10-10 02:49:22.0226 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 11/15
2025-10-10 02:49:22.0227 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 12/15
2025-10-10 02:49:22.0227 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 13/15
2025-10-10 02:49:22.0227 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 14/15
2025-10-10 02:49:22.0228 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 15/15
2025-10-10 02:49:22.0305 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2025-10-10 02:49:22.0314 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2025-10-10 02:49:22.0347 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2025-10-10 02:49:22.0350 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-10-10 02:49:22.0375 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-10-10 02:49:22.0458 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2025-10-10 02:49:22.0459 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2025-10-10 02:49:22.0479 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2025-10-10 02:49:22.0480 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2025-10-10 02:49:22.0480 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2025-10-10 02:49:22.0482 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2025-10-10 02:49:22.0484 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-10-10 02:49:22.0505 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-10-10 02:49:22.0612 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2025-10-10 02:49:22.0613 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2025-10-10 02:49:22.0626 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2025-10-10 02:49:22.0629 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-10-10 02:49:22.0638 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-10-10 02:49:22.0646 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-10-10 02:49:22.0689 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2025-10-10 02:49:22.0689 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2025-10-10 02:49:22.0716 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2025-10-10 02:49:22.0718 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2025-10-10 02:49:22.0727 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-10-10 02:49:22.0736 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2025-10-10 02:49:22.0777 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 8
2025-10-10 02:49:22.0963 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\cache\community_reporting
2025-10-10 02:52:12.0730 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2025-10-10 02:52:12.0746 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2025-10-10 02:52:12.0747 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2025-10-10 02:52:12.0769 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2025-10-10 02:52:12.0770 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity.description', 'community.full_content', 'text_unit.text']
2025-10-10 02:52:12.0772 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2025-10-10 02:52:12.0784 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2025-10-10 02:52:12.0792 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2025-10-10 02:52:12.0805 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2025-10-10 02:52:12.0805 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding entity.description: default-entity-description
2025-10-10 02:52:12.0868 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-10-10 02:52:12.0871 - INFO - graphrag.storage.file_pipeline_storage - Creating file storage at C:\Users\zhour\Desktop\christmas\cache\text_embedding
2025-10-10 02:52:12.0897 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 8 inputs via 8 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2025-10-10 02:52:19.0224 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2025-10-10 02:52:19.0615 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
2025-10-10 02:52:19.0623 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-10-10 02:52:19.0630 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 1 inputs via 1 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2025-10-10 02:52:19.0786 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2025-10-10 02:52:19.0874 - INFO - graphrag.index.operations.embed_text.embed_text - using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
2025-10-10 02:52:19.0883 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 500 to vector store
2025-10-10 02:52:19.0987 - INFO - graphrag.index.operations.embed_text.strategies.openai - embedding 42 inputs via 42 snippets using 7 batches. max_batch_size=16, batch_max_tokens=8191
2025-10-10 02:52:20.0852 - INFO - graphrag.logger.progress - generate embeddings progress: 1/7
2025-10-10 02:52:21.0623 - INFO - graphrag.logger.progress - generate embeddings progress: 2/7
2025-10-10 02:52:22.0338 - INFO - graphrag.logger.progress - generate embeddings progress: 3/7
2025-10-10 02:52:23.0089 - INFO - graphrag.logger.progress - generate embeddings progress: 4/7
2025-10-10 02:52:23.0811 - INFO - graphrag.logger.progress - generate embeddings progress: 5/7
2025-10-10 02:52:24.0544 - INFO - graphrag.logger.progress - generate embeddings progress: 6/7
2025-10-10 02:52:25.0252 - INFO - graphrag.logger.progress - generate embeddings progress: 7/7
2025-10-10 02:52:25.0462 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow completed: generate_text_embeddings
2025-10-10 02:52:25.0463 - INFO - graphrag.api.index - Workflow generate_text_embeddings completed successfully
2025-10-10 02:52:25.0534 - INFO - graphrag.index.run.run_pipeline - Indexing pipeline complete.
2025-10-10 02:52:25.0568 - INFO - graphrag.cli.index - All workflows completed successfully.
